{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.corpus import brown\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "### Compile a list of static links (permalinks) to individual user movie reviews from one particular website. This will be your working dataset for this assignment, as well as for assignments 7 and 8, which together will make up your semester project.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It does not matter if you use a crawler or if you manually collect the links, but you will need at least 100 movie review links. Note that, as of this writing, the robots.txt file of IMDB.com allows the crawling of user reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each link should be to a web page that has only one user review of only one movie, e.g., the user review permalinks on the IMDB site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose reviews of movies that are all in the same genre, e.g., sci-fi, mystery, romance, superhero, etc.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure your collection includes reviews of several movies in your chosen genre and that it includes a mix of negative and positive reviews.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PageScraper:\n",
    "    def __init__(self, url='https://www.rogerebert.com/reviews/page/2'):\n",
    "        self.urlbase = url\n",
    "        self.download_template = \"https://www.rogerebert.com/reviews/{}\"\n",
    "        self.folder = \"reviews\"\n",
    "        if self.folder not in os.listdir():\n",
    "            os.mkdir(self.folder)\n",
    "        self.pages = {}\n",
    "        \n",
    "    def get_page_names(self):\n",
    "        if \"base_page\" not in os.listdir(self.folder):\n",
    "            main_page = str(requests.get(self.urlbase).text)\n",
    "        else:\n",
    "            main_page = open(self.folder + \"/base_page\")\n",
    "        page_soup = BeautifulSoup(main_page, 'html.parser')\n",
    "        \n",
    "#         for i in page_soup.find_all(\"figcaption\"):\n",
    "        for i in page_soup.find_all(\"figure\", attrs={'class': 'movie review'}):\n",
    "            try:\n",
    "                href = i.find_all('a')[1]['href'].split('/')[2]\n",
    "                rating = len(i.find_all(\"i\", attrs={'class': 'icon-star-full'})) + .5 *\\\n",
    "                                    len(i.find_all(\"i\", attrs={'class': 'icon-star-half'}))\n",
    "                self.pages[\"{}__{}\".format(href, rating)] = \\\n",
    "                                self.download_template.format(href)\n",
    "            except IndexError:\n",
    "                continue\n",
    "                \n",
    "    def save_page(self, contents, bookname):\n",
    "        open(self.folder + \"/\" + bookname, \"w\").write(contents)\n",
    "    \n",
    "    def get_all_pages(self, sleep_time=1.5):\n",
    "        for pagename, pageurl in self.pages.items():\n",
    "            if pagename not in os.listdir(self.folder):\n",
    "                contents = str(requests.get(pageurl).text)\n",
    "                self.save_page(contents, pagename)\n",
    "                time.sleep(sleep_time)\n",
    "            \n",
    "    def check_genre(self, genre=\"Comedy\"):\n",
    "        for i in os.listdir(self.folder):\n",
    "            page = open(\"{}/{}\".format(self.folder, i)).read()\n",
    "            page_soup = BeautifulSoup(page, \"html.parser\")\n",
    "            genre_tag = page_soup.find(\"p\", attrs={'class':'genres'})\n",
    "            try:\n",
    "                if genre not in genre_tag.getText():\n",
    "                    os.remove(\"{}/{}\".format(self.folder, i))\n",
    "            except AttributeError as e:\n",
    "                os.remove(\"{}/{}\".format(self.folder, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Scraper = PageScraper()\n",
    "Scraper.get_page_names()\n",
    "Scraper.get_all_pages()\n",
    "# Scraper.check_genre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pg = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    Scraper = PageScraper(url='https://www.rogerebert.com/reviews/page/{}'.format(pg))\n",
    "    Scraper.get_page_names()\n",
    "    Scraper.get_all_pages()\n",
    "    Scraper.check_genre()\n",
    "    if len(os.listdir(Scraper.folder)) > 130:\n",
    "        break\n",
    "    else:\n",
    "        pg += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract noun phrase (NP) chunks from your reviews using the following procedure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Python, use BeautifulSoup to grab the main review text from each link.  \n",
    "\n",
    "\n",
    "\n",
    "### Next run each review text through a tokenizer, and then try to NP-chunk it with a shallow parser. \n",
    "\n",
    "\n",
    "\n",
    "### You probably will have too many unknown words, owing to proper names of characters, actors, and so on that are not in your working dictionary. Make sure the main names that are relevant to the movies in your collection of reviews are added to the working lexicon, and then run the NP chunker again.\n",
    "\n",
    "\n",
    "\n",
    "### Output all the chunks in a single list for each review, and submit that output for this assignment. Also submit a brief written summary of what you did (describe your selection of genre, your source of reviews, how many you collected, and by what means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Some code borrowed from https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "## to build a Chunker \n",
    "\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "data = conll2000.chunked_sents()\n",
    "train_data = data\n",
    "\n",
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "\n",
    "wtc = tree2conlltags(train_data[1])\n",
    "\n",
    "def conll_tag_chunks(chunk_sents):\n",
    "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
    "\n",
    "\n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff \n",
    "\n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "# define the chunker class\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_sentences, \n",
    "        tagger_classes=[UnigramTagger, BigramTagger]):\n",
    "        train_sent_tags = conll_tag_chunks(train_sentences)\n",
    "        self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
    "\n",
    "    def parse(self, tagged_sentence):\n",
    "        if not tagged_sentence: \n",
    "            return None\n",
    "        pos_tags = [tag for word, tag in tagged_sentence]\n",
    "        chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
    "        chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
    "        wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag)\n",
    "                         in zip(tagged_sentence, chunk_tags)]\n",
    "        return conlltags2tree(wpc_tags)\n",
    "  \n",
    "# train chunker model  \n",
    "ntc = NGramTagChunker(train_data)\n",
    "\n",
    "# evaluate chunker model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deploy the Chunker\n",
    "chunker = NGramTagChunker(train_data)\n",
    "\n",
    "for review in os.listdir(\"reviews\")[0]:\n",
    "    noun_parts = []\n",
    "#     print(i)\n",
    "    page = open(\"reviews/{}\".format(review)).read()\n",
    "    page_soup = BeautifulSoup(page, \"html.parser\")\n",
    "    item = page_soup.find(\"div\", attrs={'itemprop':'reviewBody'})\n",
    "    review_text = ' '.join(subitem.get_text().strip() for subitem in item.find_all('p')).strip()\n",
    "    review_text = re.sub(\" Advertisement \", '', review_text)\n",
    "    for sent in sent_tokenize(review_text):\n",
    "        sent = re.sub(\"[^a-zA-z0-9 \\-\\']\", '', sent)\n",
    "        tokens = word_tokenize(sent)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        entities = chunker.parse(tagged)\n",
    "        noun_part = ''\n",
    "        for token in entities:\n",
    "            try:\n",
    "                if token.label() == \"NP\":\n",
    "                    try:\n",
    "                        noun_part += \" \" + tuple(token)[0]\n",
    "                    except TypeError:\n",
    "                        noun_part += \" \" + \" \".join([j[0] for j in tuple(token)])\n",
    "                else:\n",
    "                    if len(noun_part) > 0:\n",
    "                        noun_parts.append(noun_part)\n",
    "                        noun_part = ''\n",
    "            except AttributeError:\n",
    "                if len(noun_part) > 0:\n",
    "                    noun_parts.append(noun_part)\n",
    "                    noun_part = ''\n",
    "            print(noun_part)\n",
    "    open(\"verb_parts/{}\".format(review), 'w').write(\"\\n\".join(noun_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## List all books which noun parts were extracted from\n",
    "all_books = os.listdir(\"noun_parts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rankings = pd.Series({book.split('__')[0]: book.split('__')[1] for book in all_books}).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "holmes-and-watson-2018                        0.5\n",
       "flower-2018                                   0.5\n",
       "show-dogs-2018                                0.5\n",
       "the-last-sharknado-its-about-time-2018        1.0\n",
       "sierra-burgess-is-a-loser-2018                1.0\n",
       "a-happening-of-monumental-proportions-2018    1.0\n",
       "arizona-2018                                  1.5\n",
       "the-con-is-on-2018                            1.5\n",
       "welcome-to-marwen-2018                        1.5\n",
       "overboard-2018                                1.5\n",
       "dtype: object"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Looking at the most poorly rated comedy movies\n",
    "rankings.sort_values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' the character',\n",
       " ' Some',\n",
       " ' these films',\n",
       " ' goodBilly Wilders',\n",
       " ' ambitious The Private Life',\n",
       " ' Sherlock Holmes',\n",
       " ' the brilliant',\n",
       " ' little-seen cult classic Zero Effectand some',\n",
       " ' them such',\n",
       " ' Gene Wilders The Adventures',\n",
       " ' Sherlock Holmes Smarter Brother',\n",
       " ' the Michael Caine romp',\n",
       " ' a Clue',\n",
       " ' all',\n",
       " ' those cases good']"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using the title \"Holmes-and-watson-2018\" which got a 0.5 star rating\n",
    "open('noun_parts/holmes-and-watson-2018__0.5').read().split(\"\\n\")[50:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "let-the-sunshine-in-2018                              4.0\n",
       "private-life-2018                                     4.0\n",
       "the-ballad-of-buster-scruggs-2018                     4.0\n",
       "blindspotting-2018                                    4.0\n",
       "blackkklansman-2018                                   4.0\n",
       "the-other-side-of-the-wind-2018                       4.0\n",
       "a-bread-factory-part-two-walk-with-me-a-while-2018    4.0\n",
       "a-bread-factory-part-one-2018                         4.0\n",
       "the-old-man-and-the-gun-2018                          3.5\n",
       "spider-man-into-the-spider-verse-2018                 3.5\n",
       "dtype: object"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Looking at the most highly rated Comedy Movies\n",
    "rankings.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' who-slept-with-who gossip',\n",
       " ' gallery owners',\n",
       " ' Vincent',\n",
       " ' spectacular relish',\n",
       " ' Xavier Beauvois the actor',\n",
       " ' filmmaker best',\n",
       " ' 2010s',\n",
       " ' Gods',\n",
       " ' Men',\n",
       " ' a real piece',\n",
       " ' work arrogant',\n",
       " ' end a',\n",
       " ' pedantic',\n",
       " ' whiskey snob theyre worse',\n",
       " ' wine snobs']"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('noun_parts/let-the-sunshine-in-2018__4.0').read().split(\"\\n\")[50:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets see if there's a clear distinction between the nouns used in reviews of highly rated movies vs poorly rated movies\n",
    "good_movies = (rankings[rankings > 3.0].index + \"__\" + rankings[rankings > 3.0].astype(str)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_lines= []\n",
    "for mov in good_movies:\n",
    "    nps = open('noun_parts/{}'.format(mov)).read().strip().split(\"\\n\")\n",
    "    for line in nps:\n",
    "        pos_lines.append([mov, line])\n",
    "\n",
    "pos_lines = pd.DataFrame(all_lines, columns=['movie_name', \"review_line\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_lines= []\n",
    "for mov in bad_movies:\n",
    "    nps = open('noun_parts/{}'.format(mov)).read().strip().split(\"\\n\")\n",
    "    for line in nps:\n",
    "        neg_lines.append([mov, line])\n",
    "\n",
    "neg_lines = pd.DataFrame(all_lines, columns=['movie_name', \"review_line\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the easier task he   --    who we\n",
      " the tourists   --    aesthetics\n",
      " a collision course that   --    this attractive woman\n",
      " the Queens chamber   --    It\n",
      " Im   --    mutual aggravation\n",
      " them   --    hes gone\n",
      " a middle finger   --    There\n",
      " Both Adam   --    Elliots warm\n",
      " themand it   --    the array\n",
      " an intimate home   --    scene this\n",
      " Spacek thats impossible   --    a special breed\n",
      " the music he   --    a man\n",
      " beat   --    writer\n",
      " the climax unfolds   --    an\n",
      " a tender hand   --    Everything\n",
      " celebrity   --    more moved\n",
      " The figures   --    insects who\n",
      " the challenge   --    enormous\n",
      " the morningand   --    stylistic flourishes\n",
      " The town   --    it\n",
      " citation   --    it\n",
      " Gjonola sings My Bathroomthe song she   --    Miles\n",
      " It   --    the reality\n",
      " a social media stalker Parinaz Izadyar who   --    real people\n",
      " all the way   --    we\n"
     ]
    }
   ],
   "source": [
    "## Print out positive review line on the left, negative review lines on the right\n",
    "for g,b in zip(pos_lines['review_line'].sample(25).values, neg_lines.sample(25)['review_line'].values):\n",
    "    print(g, \"  --  \", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive review lines are in general much more descriptive then the negative review lines, and they also seem to be more likley to mention character names in the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since the 135 reviews included in this project are quite large, and the scraper has been optimized to avoid triggering a ddos attack warning, this code base takes quite long to run.  The html documents and review contents can be found on my github at https://github.com/tdeason416/DS337/tree/master/Week7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

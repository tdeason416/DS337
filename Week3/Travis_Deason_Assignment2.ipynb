{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.book import *\n",
    "from nltk.corpus import words\n",
    "import re\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    if text is str:\n",
    "        text = split_text(remove_meta(text))\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "def vocab_count(text):\n",
    "    if text is str:\n",
    "        text = split_text(remove_meta(text))\n",
    "    return len(set(text))\n",
    "\n",
    "def split_text(text):\n",
    "    return re.split(\"[^a-z']+\")\n",
    "\n",
    "def remove_meta(text):\n",
    "    return re.split(end_line, re.split(start_line, text)[1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) In Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. It does not matter what method you use for normalization as long as you explain it in a short paragraph. (Various methods will be discussed in the live session.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = max([\n",
    "    vocab_score(text1),\n",
    "    vocab_score(text2),\n",
    "    vocab_score(text3),\n",
    "    vocab_score(text4),\n",
    "    vocab_score(text5),\n",
    "    vocab_score(text6),\n",
    "    vocab_score(text7),\n",
    "    vocab_score(text8),\n",
    "    vocab_score(text9)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19317"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_score(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Moby Dick by Herman Melville 1851'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The method listed above takes all the texts from the nltk text database and uses the max word count in any of those documents as the value of 1.  This book had a total number of unique words of 19317.  This book was Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import BigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must specify either training data or trained model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9151c08ec4cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigramTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/travis/anaconda/lib/python3.6/site-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train, model, backoff, cutoff, verbose)\u001b[0m\n\u001b[1;32m    381\u001b[0m                  backoff=None, cutoff=0, verbose=False):\n\u001b[1;32m    382\u001b[0m         NgramTagger.__init__(self, 2, train, model,\n\u001b[0;32m--> 383\u001b[0;31m                              backoff, cutoff, verbose)\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_json_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/travis/anaconda/lib/python3.6/site-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n, train, model, backoff, cutoff, verbose)\u001b[0m\n\u001b[1;32m    285\u001b[0m                  backoff=None, cutoff=0, verbose=False):\n\u001b[1;32m    286\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mContextTagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/travis/anaconda/lib/python3.6/site-packages/nltk/tag/api.py\u001b[0m in \u001b[0;36m_check_params\u001b[0;34m(self, train, model)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             raise ValueError(\n\u001b[0;32m---> 77\u001b[0;31m                     'Must specify either training data or trained model.')\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must specify either training data or trained model."
     ]
    }
   ],
   "source": [
    "tagger = BigramTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236736"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab_score(text):\n",
    "    return vocab_count(text) / len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08159722222222222"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_score(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A more useful method would be to compare the number of total words in a document to the 236,376 words found in the nltk.corpus.words.words() object.  This is the method I will be using for the remaider of the Document, where a book which contains 236,376 unique words will have a vocabulary score of 1, and a book which contains only 5 unique words will have a value of 5 / 236,376"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def longword_count(text, min_length=15):\n",
    "    '''\n",
    "    Counts the number of unique long words in a body of text\n",
    "    --------\n",
    "    INPUTS\n",
    "    text: {str | list}\n",
    "        -   The Body of text to check for long words\n",
    "    min_length: int (default 15)\n",
    "        -   Minimum word length to be considered a 'long word'\n",
    "    --------\n",
    "    RETURNS\n",
    "    vocab_score: int\n",
    "        -   number of total words in text which are at least min_length\n",
    "    '''\n",
    "    if text is str:\n",
    "        text = split_text(text)\n",
    "    longwords = []\n",
    "    for word in text:\n",
    "        try:\n",
    "            word[min_length - 1]\n",
    "            longwords.append(word)\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return vocab_count(longwords)\n",
    "\n",
    "def longword_score(text, min_length=15):\n",
    "    '''\n",
    "    Scores the number of unique long words in a body of text when\n",
    "    compared to the total number of words of equal lenght in nltk.corpus.words.words()\n",
    "    --------\n",
    "    INPUTS\n",
    "    text: {str | list}\n",
    "        -   The Body of text to check for long words\n",
    "    min_length: int (default 15)\n",
    "        -   Minimum word length to be considered a 'long word'\n",
    "    --------\n",
    "    RETURNS\n",
    "    longwords_score: float\n",
    "        -   count of total words > minwords / number of words > min_length in nltk.corpus.words.words()\n",
    "    '''\n",
    "    longwords_count = 0\n",
    "    for word in all_words:\n",
    "        try: \n",
    "            word[min_length - 1]\n",
    "            longwords_count += 1\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return float(longword_count(text, min_length)) / longwords_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longword_count(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0073852922690132"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longword_score(text5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The function 'longword_count' checks all words in a corpus of text to see if the words are at least 15 characters.  If the word is at least 15 characters, it adds them to a list and counts the total number of uniuqe words.  The function longword_score performs the function in longword_count, but it also counts the total number of long words in nltk.corpus.words.words() and compares the number of long words in the text to that value (with the number of total long words in nltk.corpus being a value of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts you used in homework 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The Bacon Second Reader\n",
    "book1 = requests.get('http://www.gutenberg.org/cache/epub/15659/pg15659.txt').content\n",
    "time.sleep(6)\n",
    "## Mcguffy’s Forth Eclectic Reader\n",
    "book2 = requests.get('http://www.gutenberg.org/cache/epub/14880/pg14880.txt').content\n",
    "time.sleep(4)\n",
    "## Mcguffy’s Fifth Eclectic Reader\n",
    "book3 = requests.get('http://www.gutenberg.org/cache/epub/15040/pg15040.txt').content\n",
    "time.sleep(5)\n",
    "## The Ontario High School Reader\n",
    "book4 = requests.get('http://www.gutenberg.org/cache/epub/19923/pg19923.txt').content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_score(text):\n",
    "    return (lexical_diversity(text) + vocab_score(text) + longword_score(text)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bacon Second Reader:  0.09076099505319624\n",
      "Mcguffy’s Forth Eclectic Reader:  0.057444635626472146\n",
      "Mcguffy’s Fifth Eclectic Reader:  0.04989201175903839\n",
      "The Ontario High School Reader:  0.0478885863745599\n"
     ]
    }
   ],
   "source": [
    "print(\"The Bacon Second Reader: \", total_score(str(book1)))\n",
    "print(\"Mcguffy’s Forth Eclectic Reader: \", total_score(str(book2)))\n",
    "print(\"Mcguffy’s Fifth Eclectic Reader: \", total_score(str(book3)))\n",
    "print(\"The Ontario High School Reader: \", total_score(str(book4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data here still shows that the second grade reader will have the highest difficulty score of all of the listed books.  The reason for this is that the normalization parameters for vocab score and longword score are dominated by lexical diversity (because this value is much larger) A good way of fixing this would be to either adjust the normalization parameter (the majority of all english language may be a bit extreme for a single book to encompass) or to utilize a non-linear scale for these values, as the more complex a vocabulary gets, the less significent adding a couple of extra words will be to its overall complexity.  This could be a strong application of utilizing a non-linear scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_score_sqrt(text):\n",
    "    return (lexical_diversity(text) \\\n",
    "            + np.sqrt(vocab_score(text)) \\\n",
    "            + np.sqrt(longword_score(text))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bacon Second Reader:  1.93896000972\n",
      "Mcguffy’s Forth Eclectic Reader:  1.92709169301\n",
      "Mcguffy’s Fifth Eclectic Reader:  1.91953906914\n",
      "The Ontario High School Reader:  1.9387259395\n"
     ]
    }
   ],
   "source": [
    "print(\"The Bacon Second Reader: \", total_score_sqrt(str(book1)))\n",
    "print(\"Mcguffy’s Forth Eclectic Reader: \", total_score_sqrt(str(book2)))\n",
    "print(\"Mcguffy’s Fifth Eclectic Reader: \", total_score_sqrt(str(book3)))\n",
    "print(\"The Ontario High School Reader: \", total_score_sqrt(str(book4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using a square root scale, all the values end up looking about the same.  It would be possible to tune these values to match the expected outcome, but there is a large risk of overfitting the data; so untill we are planning on developing a difficulty score for a much larger corpus of labeled text, it would probably be a better idea to not assume difficulty is going to be a strictly evenly weighted linear combination of the three scores we have so far obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import edit_distance\n",
    "from nltk import pos_tag\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.corpus import brown\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "## Compare your given name with your nickname (if you don’t have a nickname, invent one for this assignment) by answering the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = \"Travis\"\n",
    "nickname = \"Chavez\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the edit distance between your nickname and your given name?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "T != C\n",
    "r != h\n",
    "a == a\n",
    "v == v\n",
    "i != e\n",
    "s != z\n",
    "\n",
    "2 characters are the same, 4 need to be changes, lengths are equal.\n",
    "The edit distance is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance(name, nickname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the percentage string match between your nickname and your given name?\n",
    "\n",
    "Show your work for both calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(name))\n",
    "print(len(nickname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name and nickname are 0.33% similar\n"
     ]
    }
   ],
   "source": [
    "## Since they are of equal length, we can zip the two together\n",
    "matches = 0\n",
    "\n",
    "for i,j in zip(name, nickname):\n",
    "    if i == j:\n",
    "        matches += 1\n",
    "        \n",
    "print(\"Name and nickname are {:.2f}% similar\".format(float(matches) / len(name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find a friend (or family member or classmate) who you know has read a certain book. Without your friend knowing, copy the first two sentences of that book. Now rewrite the words from those sentences, excluding stop words. Now tell your friend to guess which book the words are from by reading them just that list of words. Did you friend correctly guess the book on the first try? What did he or she guess? Explain why you think you friend either was or was not able to guess the book from hearing the list of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mbdick = \"Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Yes, the person did guess the book.  I believe this is because the first sentence of this book is very well known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [i for i in re.split(\"[^a-zA-z]\", mbdick) if not re.search(\"[^a-zA-z]\", i) and len(i) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mbdick_nostop = [word for word in word_tokenize(mbdick) if word.lower() not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Call Ishmael . years ago—never mind long precisely—having little money purse , nothing particular interest shore , thought would sail little see watery part world .'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(mbdick_nostop)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The person was still able to guess the book because the first sentence is mostly intact.  \"Call Ishmael\" contains nearly as much information as \"Call me Ishmael\".  The 2nd sentence, on the other hand, gets to be rather confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run one of the stemmers available in Python. Run the same two sentences from question 2 above through the stemmer and show the results. How many of the outputted stems are valid morphological roots of the corresponding words? Express this answer as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86% of the words stemmed from in the first sentence of Moby Dick using a Porter Stemmer are valid roots\n",
      "0.77% of the words stemmed from in the first sentence of Moby Dick using a Lancaster Stemmer are valid roots\n",
      "0.86% of the words stemmed from in the first sentence of Moby Dick using a Snowball Stemmer are valid roots\n"
     ]
    }
   ],
   "source": [
    "Porter = PorterStemmer()\n",
    "p_stems = [Porter.stem(i) for i in tokens]\n",
    "\n",
    "valid_counts = 0\n",
    "\n",
    "for i in p_stems:\n",
    "    if i in all_words:\n",
    "        valid_counts += 1\n",
    "    \n",
    "frmt = \"{:.2f}% of the words stemmed from in the first sentence of Moby Dick using a Porter Stemmer are valid roots\"\n",
    "\n",
    "print(frmt.format(valid_counts / len(p_stems)))\n",
    "\n",
    "Lancaster = LancasterStemmer()\n",
    "l_stems = [Lancaster.stem(i) for i in tokens]\n",
    "\n",
    "valid_counts = 0\n",
    "\n",
    "for i in l_stems:\n",
    "    if i in all_words:\n",
    "        valid_counts += 1\n",
    "    \n",
    "frmt = \"{:.2f}% of the words stemmed from in the first sentence of Moby Dick using a Lancaster Stemmer are valid roots\"\n",
    "\n",
    "print(frmt.format(valid_counts / len(l_stems)))\n",
    "\n",
    "Snowball = SnowballStemmer(\"english\")\n",
    "s_stems = [Snowball.stem(i) for i in tokens]\n",
    "\n",
    "valid_counts = 0\n",
    "\n",
    "for i in s_stems:\n",
    "    if i in all_words:\n",
    "        valid_counts += 1\n",
    "    \n",
    "frmt = \"{:.2f}% of the words stemmed from in the first sentence of Moby Dick using a Snowball Stemmer are valid roots\"\n",
    "\n",
    "print(frmt.format(valid_counts / len(s_stems)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run one of the part-of-speech (POS) taggers available in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# long_sent = \"Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.\"\n",
    "# long_sent = \"In the loveliest town of all, where the houses were white and high and the elms trees were green and higher than the houses, where the front yards were wide and pleasant and the back yards were bushy and worth finding out about, where the streets sloped down to the stream and the stream flowed quietly under the bridge, where the lawns ended in orchards and the orchards ended in fields and the fields ended in pastures and the pastures climbed the hill and disappeared over the top toward the wonderful wide sky, in this loveliest of all towns Stuart stopped to get a drink of sarsaparilla.\"\n",
    "long_sent = \"The building owners are conducting an investigation alongside local authorities and expect to resume production today.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the longest sentence you can, longer than 10 words, that the POS tagger tags correctly. Show the input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "long_tokens = [i for i in re.split(\"[^a-zA-z]\", long_sent) if not re.search(\"[^a-zA-z]\", i) and len(i) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The building owners are conducting an investigation alongside local authorities and expect to resume production today.\n"
     ]
    }
   ],
   "source": [
    "print(long_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = UnigramTagger(brown.tagged_sents(categories='news'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('building', 'NN'),\n",
       " ('owners', 'NNS'),\n",
       " ('are', 'BER'),\n",
       " ('conducting', 'VBG'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('alongside', 'IN'),\n",
       " ('local', 'JJ'),\n",
       " ('authorities', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('expect', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('resume', None),\n",
       " ('production', 'NN'),\n",
       " ('today', 'NR')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(long_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the shortest sentence you can, shorter than 10 words, that the POS tagger fails to tag 100 percent correctly. Show the input and output. Explain your conjecture as to why the tagger might have been less than perfect with this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_sent = \"The committee will table the proposal until additional funding is avalaible\"\n",
    "short_tokens = [i for i in re.split(\"[^a-zA-z]\", short_sent) if not re.search(\"[^a-zA-z]\", i) and len(i) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('committee', 'NN'),\n",
       " ('will', 'MD'),\n",
       " ('table', 'NN'),\n",
       " ('the', 'AT'),\n",
       " ('proposal', 'NN'),\n",
       " ('until', 'IN'),\n",
       " ('additional', 'JJ'),\n",
       " ('funding', None),\n",
       " ('is', 'BEZ'),\n",
       " ('avalaible', None)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(short_tokens)\n",
    "# pos_tag(short_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The POS tagger was unable to reconize the word \"table\" was being used as a verb in this case, this is because the word \"table\" is typically used as a noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a different POS tagger in Python. Process the same two sentences from question 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('building', 'NN'),\n",
       " ('owners', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('conducting', 'VBG'),\n",
       " ('an', 'DT'),\n",
       " ('investigation', 'NN'),\n",
       " ('alongside', 'RB'),\n",
       " ('local', 'JJ'),\n",
       " ('authorities', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('expect', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('resume', 'VB'),\n",
       " ('production', 'NN'),\n",
       " ('today', 'NN')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(long_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('committee', 'NN'),\n",
       " ('will', 'MD'),\n",
       " ('table', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('proposal', 'NN'),\n",
       " ('until', 'IN'),\n",
       " ('additional', 'JJ'),\n",
       " ('funding', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('avalaible', 'JJ')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(short_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it produce the same or different output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "using NLTK's default stemmer, which considers adjacent words, \"table\" is correctly identified as a verb.  The other sentence is still correctly identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain any differences as best you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Despite it's length, the first sentence is fairly simple in composition.  All of the words are used in their most frequent part of speech.  The unigram tagger therefore chooses the most common part of speech for each word, and it turns out correctly.  The second sentence, on the other hand uses the word \"table\" as a verb.  Since the most common usage for \"table\" is as a noun, the Unigram tagger is unable to correctly identify it's part of speech.  This is because it is not considering the sentence as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a news article from this week’s news, find a random sentence of at least 10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_sent = \"Major fireworks display, entertainment and an address by your favorite President, me!\"\n",
    "news_tokens = [i for i in re.split(\"[^a-zA-z]\", news_sent) if not re.search(\"[^a-zA-z]\", i) and len(i) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the Penn tag set, manually POS tag the sentence yourself."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Major -- JJ\n",
    "fireworks -- NNS\n",
    "display, -- NN\n",
    "entertainment -- NN\n",
    "and -- CC\n",
    "an -- IN\n",
    "address -- noun \n",
    "by -- IN\n",
    "your -- PRP\n",
    "favorite -- JJ\n",
    "President, -- NN \n",
    "me! -- PRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now run the same sentences through both taggers that you implemented for questions 1 and 2. Did either of the taggers produce the same results as you had created manually?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Major', 'NN-TL'),\n",
       " ('fireworks', None),\n",
       " ('display', 'NN'),\n",
       " ('entertainment', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('an', 'AT'),\n",
       " ('address', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('your', 'PP$'),\n",
       " ('favorite', 'JJ'),\n",
       " ('President', 'NN-TL'),\n",
       " ('me', 'PPO')]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(news_tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Unigram tagger gives very diffirent results then I did.  I will outline all the discrepencies below:\n",
    "\n",
    "Major - This is being used as an adjective but the unigram tagger was probably confused by it being capatilized, and labeled the tag to be a member of the armed services\n",
    "\n",
    "fireworks - This word may have not been in the unigram vocabulary.\n",
    "\n",
    "an - the unigram tagger got this one right, this was my mistake calling it a preposition\n",
    "\n",
    "me - I could not find the \"PPO\" tag in the Penn-Treebank tagset.  I am not sure how the program came up with this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Major', 'JJ'),\n",
       " ('fireworks', 'NNS'),\n",
       " ('display', 'VBP'),\n",
       " ('entertainment', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('an', 'DT'),\n",
       " ('address', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('your', 'PRP$'),\n",
       " ('favorite', 'JJ'),\n",
       " ('President', 'NNP'),\n",
       " ('me', 'PRP')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(news_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain any differences between the two taggers and your manual tagging as much as you can."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The pos_tag function from nltk mostly agrees with the manual tags I applied.  The major exception being the word \"display\"  Pos_tag mistakingly considers this word to be a verb while it is atcually being used as a noun in this sentence.  The sentence structure is a bit odd, so it is unsuprising that there are mistakes.  Additionally the word \"an\" is labeled as a \"Determiner\" by nltk and a Preposition by me.  This is probably my mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3-years', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('experience', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('Finite', 'NNP'),\n",
       " ('Element', 'NNP'),\n",
       " ('Modeling', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('10,000-psi', 'JJ'),\n",
       " ('pneumatics', 'NNS')]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(\"3-years of experience in Finite Element Modeling of 10,000-psi pneumatics\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3-year', 'of', 'experi', 'in', 'finit', 'element', 'model', 'of', '10,000', 'psi', 'pneumat']\n"
     ]
    }
   ],
   "source": [
    "Snowball = SnowballStemmer(\"english\")\n",
    "print([Snowball.stem(i) for i in \"3-years of experience in finite element modeling of 10,000 psi pneumatics\".split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmed = [Snowball.stem(i) for i in \"3-years of experience in finite element modeling of 10,000 psi pneumatics\".split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoppped = [w for w in stemmed if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3-year', 'JJ'),\n",
       " ('experi', 'NN'),\n",
       " ('finit', 'NN'),\n",
       " ('element', 'JJ'),\n",
       " ('model', 'NN'),\n",
       " ('10,000', 'CD'),\n",
       " ('psi', 'NN'),\n",
       " ('pneumat', 'NN')]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(stoppped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
